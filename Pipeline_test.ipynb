{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d199e606-35a1-4031-83a8-315185333335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import requests\n",
    "import glob, os, math\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer, AutoModel\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from utils.top_accuracies import calculate_top_n_accuracies, topk, topN\n",
    "\n",
    "from utils.extract_sequence import extract_sequence\n",
    "from utils.pocket_feature import pocket_feature\n",
    "from utils.sequence_indices import sequence_indices\n",
    "from utils.pocket_coordinates import pocket_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dcd9f15-7124-433f-ba2b-9bffe7ac686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ATOMS = 9\n",
    "MODEL_PATH = \"/home/mkhokhar21/Documents/COSBI/Allostery_Paper/prot_bert_mtl\"\n",
    "base_url = \"https://files.rcsb.org/download\"\n",
    "pdb_dir = \"/home/mkhokhar21/Documents/COSBI/Allostery_Paper/data/pdbs/\"\n",
    "pocket_dir = \"/home/mkhokhar21/Documents/COSBI/Allostery_Paper/data/pockets/\"\n",
    "pdb_id = \"3PEE\"\n",
    "chain_id = \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aebb8c8-3e80-4056-ba14-6b4ee912ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels_task1, num_labels_task2):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.head1 = nn.Linear(self.encoder.config.hidden_size, num_labels_task1)\n",
    "        self.head2 = nn.Linear(self.encoder.config.hidden_size, num_labels_task2)\n",
    "\n",
    "    def forward(self, input1=None, input2=None):\n",
    "        output1, output2 = None, None\n",
    "        encoder_output1, encoder_output2 = None, None\n",
    "\n",
    "        if input1 is not None:\n",
    "            encoder_output1 = self.encoder(**input1).last_hidden_state\n",
    "            output1 = self.head1(encoder_output1)\n",
    "\n",
    "        if input2 is not None:\n",
    "            encoder_output2 = self.encoder(**input2).last_hidden_state\n",
    "            output2 = self.head2(encoder_output2)\n",
    "\n",
    "        return (output1, output2), (encoder_output1, encoder_output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3aa6905-a193-4f8a-98a6-648cdcd12458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_data(poc_res_emb, pocket_coord, pocket_features, labels):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(min(len(poc_res_emb), len(pocket_coord))):\n",
    "        seq_emb = []\n",
    "        for res_idx in range(min(len(poc_res_emb[i]), len(pocket_coord[i]))):\n",
    "            seq_emb.append(poc_res_emb[i][res_idx])\n",
    "        seq_emb = np.array(seq_emb).mean(axis=0)\n",
    "        poc = pocket_features[i]\n",
    "        X.append(np.concatenate((seq_emb, poc)))\n",
    "#### Test - begin ####\n",
    "        Y.append(labels[i])\n",
    "#### Test - end ####\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def do_it(pdb_id, chain_id):\n",
    "    pdb_path = os.path.join(pdb_dir, f\"{pdb_id}.pdb\")\n",
    "    pocket_path = os.path.join(pocket_dir, f\"{pdb_id}_out\")\n",
    "\n",
    "    #### Test - begin ####\n",
    "    ASD_path = \"/home/mkhokhar21/Documents/COSBI/Allostery_Paper/data/source_data/ASD_Release_201909_AS.txt\"\n",
    "\n",
    "    asd = None\n",
    "    with open(ASD_path, \"r\") as f:\n",
    "        asd = f.readlines()\n",
    "\n",
    "    mod_id, modulator, residues = None, None, None\n",
    "    for line in asd[1:]:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        pdb, modulator, chain_id, mod_id = line[4], line[6], line[7], line[11]\n",
    "\n",
    "        if pdb != pdb_id:\n",
    "            continue\n",
    "\n",
    "        if len(set(chain_id.split(\";\"))) != 1:\n",
    "            continue\n",
    "        chain_id = chain_id[0]\n",
    "\n",
    "        if len(set(modulator.split(\";\"))) != 1:\n",
    "            continue\n",
    "        modulator = modulator.split(\";\")[0]\n",
    "\n",
    "        # extract residues\n",
    "        res_raw = [\n",
    "            res.replace(\":\", \",\").split(\",\") for res in line[-1].split(\"; \")\n",
    "        ]\n",
    "        # residue_clean format: chain id + residue type + residue number\n",
    "        residues = [\n",
    "            [res[0][-1], ch[:3], ch[3:]] for res in res_raw for ch in res[1:]\n",
    "        ]\n",
    "        # select only residues in the same chain of modulator\n",
    "        residues = [res for res in residues if res[0] == chain_id]\n",
    "\n",
    "        break\n",
    "    #### Test - end ####\n",
    "\n",
    "\n",
    "    if not os.path.exists(pdb_path):\n",
    "        response = requests.get(f\"{base_url}/{pdb_id}.pdb\")\n",
    "        if response.status_code == 200:  # Check if the request was successful\n",
    "            with open(pdb_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"PDB file {pdb_id}.pdb downloaded successfully.\")\n",
    "        else:\n",
    "            raise Exception(f\"Failed to download {pdb_id}.pdb. Check if the PDB ID is correct.\")\n",
    "\n",
    "    sequence = extract_sequence(pdb_path, chain_id)\n",
    "\n",
    "    if len(sequence) <= 10:\n",
    "        raise Exception(\"Sequence is too short.\")\n",
    "\n",
    "    if not os.path.exists(pocket_path):\n",
    "        os.system(f\"fpocket -f {pdb_path} -k {chain_id}\")\n",
    "        os.system(f\"mv {os.path.join(pdb_dir, pdb_id)}_out {pocket_dir}\")\n",
    "\n",
    "    #### Test - begin ####\n",
    "    protein = None\n",
    "    lig_x, lig_y, lig_z, lig_cnt = 0, 0, 0, 0\n",
    "\n",
    "    with open(pdb_path, \"r\") as f:\n",
    "        protein = f.readlines()\n",
    "\n",
    "    for line in protein:\n",
    "        if (\n",
    "            line[:6] == \"HETATM\" and modulator == line[17:20].strip()\n",
    "            and line[21] == chain_id and mod_id == line[22:26].strip()\n",
    "        ):\n",
    "            lig_x += float(line[30:38])\n",
    "            lig_y += float(line[38:46])\n",
    "            lig_z += float(line[46:54])\n",
    "            lig_cnt += 1\n",
    "\n",
    "    lig_x /= lig_cnt\n",
    "    lig_y /= lig_cnt\n",
    "    lig_z /= lig_cnt\n",
    "    #### Test - end ####\n",
    "\n",
    "    pocket_names = glob.glob(f\"{pocket_path}/pockets/*.pdb\")\n",
    "    pocket_names = sorted(\n",
    "        pocket_names,\n",
    "        key=lambda x: int(x.split(\"pocket\")[-1].split(\"_\")[0])\n",
    "    )\n",
    "\n",
    "    pockets_feats = pocket_feature(f\"{pocket_path}/{pdb_id}_info.txt\")\n",
    "    selected_idxs = []\n",
    "    pocket_residue_indices = []\n",
    "\n",
    "    #### Test - begin ####\n",
    "    atomTarget = {}\n",
    "    for res in residues:\n",
    "        atomTarget[f'{res[1]}{res[2]}'] = res[0]\n",
    "\n",
    "    dists = []\n",
    "    countsPockets = [] # for atom count\n",
    "    #### Test - end ####\n",
    "\n",
    "    for idx, pocket_name in enumerate(pocket_names):\n",
    "        pocket = None\n",
    "        with open(pocket_name, \"r\") as f:\n",
    "            pocket = f.readlines()\n",
    "\n",
    "    #### Test - begin ####\n",
    "        poc_x, poc_y, poc_z = 0, 0, 0\n",
    "        pocketAtomCount = 0\n",
    "    #### Test - end ####\n",
    "\n",
    "        poc_cnt = 0\n",
    "        residue_indices = set()\n",
    "\n",
    "        for line in pocket:\n",
    "            if line[:4] == \"ATOM\":\n",
    "                poc_cnt += 1\n",
    "                residue_index = line[22:26].strip()\n",
    "                atom = line[17:20] + residue_index\n",
    "                residue_indices.add(residue_index)\n",
    "\n",
    "    #### Test - begin ####\n",
    "                poc_x += float(line[30:38])\n",
    "                poc_y += float(line[38:46])\n",
    "                poc_z += float(line[46:54])\n",
    "                chainID = line[21]\n",
    "                if atom in atomTarget and atomTarget[atom] == chainID:\n",
    "                    pocketAtomCount += 1\n",
    "    #### Test - end ####\n",
    "\n",
    "        if poc_cnt == 0:\n",
    "            continue\n",
    "\n",
    "    #### Test - begin ####\n",
    "        poc_x /= poc_cnt\n",
    "        poc_y /= poc_cnt\n",
    "        poc_z /= poc_cnt\n",
    "        dist = math.sqrt(\n",
    "            (poc_x - lig_x) ** 2 + (poc_y - lig_y) ** 2 +\n",
    "            (poc_z - lig_z) ** 2\n",
    "        )\n",
    "\n",
    "        dists.append(dist)\n",
    "        countsPockets.append(pocketAtomCount)\n",
    "    #### Test - end ####\n",
    "\n",
    "        selected_idxs.append(idx)\n",
    "        pocket_residue_indices.append(list(residue_indices))\n",
    "\n",
    "    if len(selected_idxs) <= 2:\n",
    "        raise Exception(\"Too few pockets extracted.\")\n",
    "\n",
    "    pocket_features = [pockets_feats[idx] for idx in selected_idxs]\n",
    "\n",
    "    seq_indices = sequence_indices(pdb_id, chain_id)\n",
    "\n",
    "    #### Test - begin ####\n",
    "    dist_min_idx = np.argmin(dists)\n",
    "    labels = [1 if item >= N_ATOMS else 0 for item in countsPockets] # for atom count\n",
    "    labels[dist_min_idx] = 1\n",
    "\n",
    "    seq_labels = ['N'] * len(sequence)\n",
    "    for i in range(len(labels)):\n",
    "            if labels[i] == 1:\n",
    "                for residue_index in pocket_residue_indices[i]:\n",
    "                    if residue_index in seq_indices and seq_indices[residue_index] < len(sequence):\n",
    "                        seq_labels[seq_indices[residue_index]] = 'Y'\n",
    "    #### Test - end ####\n",
    "\n",
    "    pocket_coord = pocket_coordinates(pdb_path, f\"{pocket_path}/pockets/\", pdb_id, chain_id, pocket_residue_indices)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd\", do_lower_case=False)\n",
    "    model = MultiTaskModel(\"Rostlab/prot_bert_bfd\", 2, 3)\n",
    "    state_dict = torch.load(\"/home/mkhokhar21/Documents/COSBI/Allostery_Paper/prot_bert_mtl/prot_bert_mtl.bin\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    poc_res_emb = []\n",
    "\n",
    "    #### Test - begin ####\n",
    "    poc_labels = []\n",
    "    #### Test - end ####\n",
    "\n",
    "    with torch.no_grad():\n",
    "        seq = \" \".join(sequence)\n",
    "        encoding = tokenizer.batch_encode_plus(\n",
    "            [seq],\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        input_ids = torch.tensor(encoding['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(encoding['attention_mask']).to(device)\n",
    "        inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        _, (last_hidden_state, _) = model(input1=inputs)\n",
    "        embedding = last_hidden_state.cpu().numpy()\n",
    "\n",
    "        seq_len = (attention_mask[0] == 1).sum()\n",
    "        token_emb = embedding[0][1:seq_len-1]\n",
    "\n",
    "        for i in range(len(pocket_residue_indices)):\n",
    "            add_pocket = True\n",
    "            cur_poc_emb = []\n",
    "\n",
    "    #### Test - begin ####\n",
    "            poc_labels.append(labels[i])\n",
    "    #### Test - end ####\n",
    "\n",
    "            for idx in pocket_residue_indices[i]:\n",
    "                try:\n",
    "                    token = token_emb[seq_indices[idx]]\n",
    "                    cur_poc_emb.append(token)\n",
    "                except Exception as e:\n",
    "                    add_pocket = False\n",
    "    #### Test - begin ####\n",
    "                    poc_labels.pop()\n",
    "    #### Test - end ####\n",
    "                    break\n",
    "\n",
    "            if add_pocket:\n",
    "                poc_res_emb.append(cur_poc_emb)\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    X_Test, Y_Test = get_res_data(poc_res_emb, pocket_coord, pocket_features, labels)\n",
    "    X_Test, Y_Test = np.array(X_Test), np.array(Y_Test)\n",
    "    test_data = np.concatenate((X_Test, Y_Test.reshape(-1, 1)), axis=1)\n",
    "    test_data = TabularDataset(test_data)\n",
    "    test_data.columns = [str(i) for i in range(1, X_Test.shape[1] + 2)]\n",
    "    label = str(X_Test.shape[1] + 1)\n",
    "    predictor = TabularPredictor.load(\"/home/mkhokhar21/Documents/COSBI/Allostery_Paper/src/AutogluonModels/MTL_All\")\n",
    "\n",
    "    y_test_label = test_data[label]\n",
    "    y_test_nolab = test_data.drop(columns=[label])\n",
    "\n",
    "    y_pred = predictor.predict_proba(y_test_nolab)\n",
    "\n",
    "    return y_pred.to_numpy()[:, 1], Y_Test, pocket_residue_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65331c3f-01c1-4e12-ab22-4ee90cabb0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/allostery/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8430039286613464, 1, ['153', '104', '232', '203', '224', '36', '57', '34', '54', '28', '249', '209', '151', '211', '32', '208', '221', '231'])\n",
      "(0.013739123940467834, 0, ['153', '46', '110', '109', '154', '202', '108', '50', '155', '47', '203', '201'])\n",
      "(0.01281462050974369, 0, ['230', '200', '198', '199', '197', '229', '211', '212', '231'])\n"
     ]
    }
   ],
   "source": [
    "y_pred, Y_Test, pocket_residue_indices = do_it(pdb_id, chain_id)\n",
    "paired = list(zip(y_pred, Y_Test, pocket_residue_indices))\n",
    "paired_sorted = sorted(paired, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "top3 = [paired_sorted[i] for i in range(min(len(paired_sorted), 3))]\n",
    "\n",
    "for top in top3:\n",
    "    print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "442770b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8430039286613464 - 1 - select :153,104,232,203,224,36,57,34,54,28,249,209,151,211,32,208,221,231\n",
      "0.013739123940467834 - 0 - select :153,46,110,109,154,202,108,50,155,47,203,201\n",
      "0.01281462050974369 - 0 - select :230,200,198,199,197,229,211,212,231\n"
     ]
    }
   ],
   "source": [
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    for res in top[-1]:\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(f\"{top[0]} - {top[1]} - {cur_res[:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977796b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27f4984f-864f-46fb-8c61-b35f55341042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../prot_bert_allosteric were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../prot_bert_allosteric and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.99999523, 1, ['574', '591', '564', '545', '635', '595', '584', '580', '636', '592', '583', '582', '593', '632', '581'])\n",
      "(0.00015850604, 0, ['631', '641', '633', '630', '640', '634', '639', '643', '638'])\n",
      "(0.00011590509, 0, ['569', '547', '570', '635', '636', '638', '592', '567', '566', '568'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../prot_bert_allosteric were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../prot_bert_allosteric and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9992513, 1, ['112', '111', '110', '113', '108'])\n",
      "(0.00021088084, 0, ['240', '193', '243', '244', '192', '247', '248', '196'])\n",
      "(0.00018359804, 0, ['139', '206', '202', '203', '249', '204', '247', '248', '199', '205'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../prot_bert_allosteric were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../prot_bert_allosteric and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9918006, 1, ['569', '280', '610', '570', '572', '612', '613', '283', '382', '611', '284', '285', '282', '281', '571', '614', '287', '770'])\n",
      "(0.98959947, 0, ['377', '574', '484', '292', '385', '341', '284', '282', '455', '672', '135', '573', '283', '383', '339', '569', '136', '286', '285', '676', '675', '674', '133', '673', '280', '88', '378', '287'])\n",
      "(0.8901069, 0, ['71', '64', '193', '240', '60', '68', '75', '67', '191', '72', '227'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../prot_bert_allosteric were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../prot_bert_allosteric and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.27388626, 1, ['36', '209', '232', '54', '221', '203', '28', '208', '231', '249', '151', '153', '211', '57', '104', '34', '224', '32'])\n",
      "(0.010215629, 0, ['139', '136', '135', '91', '37', '103', '93', '105'])\n",
      "(0.0075611584, 0, ['185', '144', '183', '143', '141', '140', '186', '184'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../prot_bert_allosteric were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../prot_bert_allosteric and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.99999034, 1, ['257', '40', '247', '112', '111', '253', '249', '114', '113', '254', '43', '39', '248', '116', '117'])\n",
      "(0.0047794143, 0, ['86', '23', '137', '197', '196', '222', '143', '198', '159', '172', '106', '142', '223', '138', '170', '160', '13', '224', '192', '108', '109', '225', '139', '174', '221', '144'])\n",
      "(0.0007168664, 0, ['8', '86', '106', '83', '6', '194', '108', '24', '80', '9', '85'])\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "pdbs = [\"1Q5O\", \"2FPL\", \"3BCR\", \"3PEE\", \"4HO6\"]\n",
    "for pdb in pdbs:\n",
    "    result.append({\"pdb\": pdb, \"top3\": do_it(pdb, \"A\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04f20e1-15e2-45c1-8f47-dd1da42927f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8491735458374023 - 1 - select :580,593,564,632,581,583,591,592,635,574,582,595,545,636,584\n",
      "0.028418462723493576 - 0 - select :569,547,567,568,570,638,566,592,635,636\n",
      "0.012626996263861656 - 0 - select :579,632,517,577,624,578,628,516\n"
     ]
    }
   ],
   "source": [
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    for res in top[-1]:\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(f\"{top[0]} - {top[1]} - {cur_res[:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "439e1bcb-a3d7-481a-9e65-017a696b1853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select :\n",
      "select :86,23,137,197,196,222,143,198,159,172,106,142,223,138,170,160,13,224,192,108,109,225,139,174,221,144,\n",
      "select :8,86,83,106,6,194,24,108,80,9,85,\n"
     ]
    }
   ],
   "source": [
    "first_set = set(x for x in result[4][\"top3\"][0][-1])\n",
    "for top in result[4][\"top3\"]:\n",
    "    cur_res = \"select :\"\n",
    "    cur_set = cur_set = set(x for x in top[-1])\n",
    "    cur_set = cur_set - first_set\n",
    "    for res in list(cur_set):\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(cur_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb300f7-0177-4885-a5ac-144a08fe0eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['574,591,564,545,635,595,584,580,636,592,583,582,593,632,581',\n",
       "  '631,641,633,630,640,634,639,643,638',\n",
       "  '569,547,570,635,636,638,592,567,566,568'],\n",
       " {'547', '566', '567', '568', '569', '570', '638'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residues = []\n",
    "first_set = set(x for x in top3[0][-1])\n",
    "cur_set= None\n",
    "for top in top3:\n",
    "    cur_res = 'select :'\n",
    "    cur_set = set(x for x in top[-1])\n",
    "    cur_set = cur_set - first_set\n",
    "    for res in top[-1]:\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    residues.append(cur_res[:-1])\n",
    "    \n",
    "residues, cur_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8920f858-74ee-40d3-b844-316bcfbe4dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../prot_bert_allosteric were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../prot_bert_allosteric and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3962158, ['303', '286', '304', '319', '348', '346', '255', '308', '350', '253', '329', '307', '287', '300', '331', '291', '290', '261', '333', '262', '317', '288'])\n",
      "(0.0033208348, ['266', '254', '265', '242', '246'])\n",
      "(0.0008981182, ['353', '321', '326', '355', '328', '327', '318', '352'])\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import requests\n",
    "import glob, os, math\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import xgboost as xgb\n",
    "\n",
    "from utils.extract_sequence import extract_sequence\n",
    "from utils.pocket_feature import pocket_feature\n",
    "from utils.sequence_indices import sequence_indices\n",
    "from utils.pocket_coordinates import pocket_coordinates\n",
    "\n",
    "N_ATOMS = 9\n",
    "MODEL_PATH = \"../prot_bert_allosteric\"\n",
    "base_url = \"https://files.rcsb.org/download\"\n",
    "pdb_dir = \"../data/pdbs/\"\n",
    "pocket_dir = \"../data/pockets/\"\n",
    "pdb_id = \"5DKK\"\n",
    "chain_id = \"A\"\n",
    "\n",
    "is_test = False\n",
    "\n",
    "pdb_path = os.path.join(pdb_dir, f\"{pdb_id}.pdb\")\n",
    "pocket_path = os.path.join(pocket_dir, f\"{pdb_id}_out\")\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    ASD_path = \"../data/source_data/ASD_Release_201909_AS.txt\"\n",
    "\n",
    "    asd = None\n",
    "    with open(ASD_path, \"r\") as f:\n",
    "        asd = f.readlines()\n",
    "\n",
    "    mod_id, modulator, residues = None, None, None\n",
    "    for line in asd[1:]:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        pdb, modulator, chain_id, mod_id = line[4], line[6], line[7], line[11]\n",
    "\n",
    "        if pdb != pdb_id:\n",
    "            continue\n",
    "\n",
    "        if len(set(chain_id.split(\";\"))) != 1:\n",
    "            continue\n",
    "        chain_id = chain_id[0]\n",
    "\n",
    "        if len(set(modulator.split(\";\"))) != 1:\n",
    "            continue\n",
    "        modulator = modulator.split(\";\")[0]\n",
    "\n",
    "        # extract residues\n",
    "        res_raw = [\n",
    "            res.replace(\":\", \",\").split(\",\") for res in line[-1].split(\"; \")\n",
    "        ]\n",
    "        # residue_clean format: chain id + residue type + residue number\n",
    "        residues = [\n",
    "            [res[0][-1], ch[:3], ch[3:]] for res in res_raw for ch in res[1:]\n",
    "        ]\n",
    "        # select only residues in the same chain of modulator\n",
    "        residues = [res for res in residues if res[0] == chain_id]\n",
    "\n",
    "        break\n",
    "#### Test - end ####\n",
    "\n",
    "\n",
    "if not os.path.exists(pdb_path):\n",
    "    response = requests.get(f\"{base_url}/{pdb_id}.pdb\")\n",
    "    if response.status_code == 200:  # Check if the request was successful\n",
    "        with open(pdb_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"PDB file {pdb_id}.pdb downloaded successfully.\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download {pdb_id}.pdb. Check if the PDB ID is correct.\")\n",
    "\n",
    "sequence = extract_sequence(pdb_path, chain_id)\n",
    "\n",
    "if len(sequence) <= 10:\n",
    "    raise Exception(\"Sequence is too short.\")\n",
    "\n",
    "if not os.path.exists(pocket_path):\n",
    "    os.system(f\"fpocket -f {pdb_path} -k {chain_id}\")\n",
    "    os.system(f\"mv {os.path.join(pdb_dir, pdb_id)}_out {pocket_dir}\")\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    protein = None\n",
    "    lig_x, lig_y, lig_z, lig_cnt = 0, 0, 0, 0\n",
    "\n",
    "    with open(pdb_path, \"r\") as f:\n",
    "        protein = f.readlines()\n",
    "\n",
    "    for line in protein:\n",
    "        if (\n",
    "            line[:6] == \"HETATM\" and modulator == line[17:20].strip()\n",
    "            and line[21] == chain_id and mod_id == line[22:26].strip()\n",
    "        ):\n",
    "            lig_x += float(line[30:38])\n",
    "            lig_y += float(line[38:46])\n",
    "            lig_z += float(line[46:54])\n",
    "            lig_cnt += 1\n",
    "\n",
    "    lig_x /= lig_cnt\n",
    "    lig_y /= lig_cnt\n",
    "    lig_z /= lig_cnt\n",
    "#### Test - end ####\n",
    "\n",
    "pocket_names = glob.glob(f\"{pocket_path}/pockets/*.pdb\")\n",
    "pocket_names = sorted(\n",
    "    pocket_names,\n",
    "    key=lambda x: int(x.split(\"pocket\")[-1].split(\"_\")[0])\n",
    ")\n",
    "\n",
    "pockets_feats = pocket_feature(f\"{pocket_path}/{pdb_id}_info.txt\")\n",
    "selected_idxs = []\n",
    "pocket_residue_indices = []\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    atomTarget = {}\n",
    "    for res in residues:\n",
    "        atomTarget[f'{res[1]}{res[2]}'] = res[0]\n",
    "\n",
    "    dists = []\n",
    "    countsPockets = [] # for atom count\n",
    "#### Test - end ####\n",
    "\n",
    "for idx, pocket_name in enumerate(pocket_names):\n",
    "    pocket = None\n",
    "    with open(pocket_name, \"r\") as f:\n",
    "        pocket = f.readlines()\n",
    "\n",
    "#### Test - begin ####\n",
    "    if is_test:\n",
    "        poc_x, poc_y, poc_z = 0, 0, 0\n",
    "        pocketAtomCount = 0\n",
    "#### Test - end ####\n",
    "\n",
    "    poc_cnt = 0\n",
    "    residue_indices = set()\n",
    "\n",
    "    for line in pocket:\n",
    "        if line[:4] == \"ATOM\":\n",
    "            poc_cnt += 1\n",
    "            residue_index = line[22:26].strip()\n",
    "            atom = line[17:20] + residue_index\n",
    "            residue_indices.add(residue_index)\n",
    "\n",
    "#### Test - begin ####\n",
    "            if is_test:\n",
    "                poc_x += float(line[30:38])\n",
    "                poc_y += float(line[38:46])\n",
    "                poc_z += float(line[46:54])\n",
    "                chainID = line[21]\n",
    "                if atom in atomTarget and atomTarget[atom] == chainID:\n",
    "                    pocketAtomCount += 1\n",
    "#### Test - end ####\n",
    "\n",
    "    if poc_cnt == 0:\n",
    "        continue\n",
    "\n",
    "#### Test - begin ####\n",
    "    if is_test:\n",
    "        poc_x /= poc_cnt\n",
    "        poc_y /= poc_cnt\n",
    "        poc_z /= poc_cnt\n",
    "        dist = math.sqrt(\n",
    "            (poc_x - lig_x) ** 2 + (poc_y - lig_y) ** 2 +\n",
    "            (poc_z - lig_z) ** 2\n",
    "        )\n",
    "\n",
    "        dists.append(dist)\n",
    "        countsPockets.append(pocketAtomCount)\n",
    "#### Test - end ####\n",
    "\n",
    "    selected_idxs.append(idx)\n",
    "    pocket_residue_indices.append(list(residue_indices))\n",
    "\n",
    "if len(selected_idxs) <= 2:\n",
    "    raise Exception(\"Too few pockets extracted.\")\n",
    "\n",
    "pocket_features = [pockets_feats[idx] for idx in selected_idxs]\n",
    "\n",
    "seq_indices = sequence_indices(pdb_id, chain_id)\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    dist_min_idx = np.argmin(dists)\n",
    "    labels = [1 if item >= N_ATOMS else 0 for item in countsPockets] # for atom count\n",
    "    labels[dist_min_idx] = 1\n",
    "\n",
    "    seq_labels = ['N'] * len(sequence)\n",
    "    for i in range(len(labels)):\n",
    "            if labels[i] == 1:\n",
    "                for residue_index in pocket_residue_indices[i]:\n",
    "                    if residue_index in seq_indices and seq_indices[residue_index] < len(sequence):\n",
    "                        seq_labels[seq_indices[residue_index]] = 'Y'\n",
    "#### Test - end ####\n",
    "\n",
    "pocket_coord = pocket_coordinates(pdb_path, f\"{pocket_path}/pockets/\", pdb_id, chain_id, pocket_residue_indices)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, do_lower_case=False )\n",
    "model = BertModel.from_pretrained(MODEL_PATH)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "seq_emb = None\n",
    "poc_res_emb = []\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    poc_labels = []\n",
    "#### Test - end ####\n",
    "\n",
    "with torch.no_grad():\n",
    "    seq = \" \".join(sequence)\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        [seq],\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    input_ids = torch.tensor(encoding['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(encoding['attention_mask']).to(device)\n",
    "    embedding = model(input_ids=input_ids, attention_mask=attention_mask)[0].cpu().numpy()\n",
    "\n",
    "    seq_len = (attention_mask[0] == 1).sum()\n",
    "    token_emb = embedding[0][1:seq_len-1]\n",
    "    seq_emb = token_emb.mean(axis=0)\n",
    "\n",
    "    for i in range(len(pocket_residue_indices)):\n",
    "        add_pocket = True\n",
    "        cur_poc_emb = []\n",
    "\n",
    "#### Test - begin ####\n",
    "        if is_test:\n",
    "            poc_labels.append(labels[i])\n",
    "#### Test - end ####\n",
    "\n",
    "        for idx in pocket_residue_indices[i]:\n",
    "            try:\n",
    "                token = token_emb[seq_indices[idx]]\n",
    "                cur_poc_emb.append(token)\n",
    "            except Exception as e:\n",
    "                add_pocket = False\n",
    "#### Test - begin ####\n",
    "                if is_test:\n",
    "                    poc_labels.pop()\n",
    "#### Test - end ####\n",
    "                break\n",
    "        \n",
    "        if add_pocket:\n",
    "            poc_res_emb.append(cur_poc_emb)\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "def get_res_data(poc_res_emb, pocket_coord):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(min(len(poc_res_emb), len(pocket_coord))):\n",
    "        seq_emb = []\n",
    "        for res_idx in range(min(len(poc_res_emb[i]), len(pocket_coord[i]))):\n",
    "            seq_emb.append(poc_res_emb[i][res_idx])\n",
    "        seq_emb = np.array(seq_emb).mean(axis=0)\n",
    "        poc = pocket_features[i]\n",
    "        X.append(np.concatenate((seq_emb, poc)))\n",
    "#### Test - begin ####\n",
    "        if is_test:\n",
    "            Y.append(labels[i])\n",
    "#### Test - end ####\n",
    "\n",
    "    if is_test:\n",
    "        return X, Y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "if is_test:\n",
    "    X_Test, Y_Test = get_res_data(poc_res_emb, pocket_coord)\n",
    "    dtest = xgb.DMatrix(X_Test, label=Y_Test)\n",
    "else:\n",
    "    X_Test = get_res_data(poc_res_emb, pocket_coord)\n",
    "    dtest = xgb.DMatrix(X_Test)\n",
    "bst = xgb.Booster()\n",
    "bst.load_model('./xgboost.model')\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "if is_test:\n",
    "    paired = list(zip(y_pred, Y_Test, pocket_residue_indices))\n",
    "else:\n",
    "    paired = list(zip(y_pred, pocket_residue_indices))\n",
    "paired_sorted = sorted(paired, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "top3 = [paired_sorted[i] for i in range(min(len(paired_sorted), 3))]\n",
    "\n",
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    for res in top[-1]:\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(f\"{top[0]} - {cur_res[:-1]}\")\n",
    "    \n",
    "first_set = set(x for x in top3[0][-1])\n",
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    cur_set = cur_set = set(x for x in top[-1])\n",
    "    cur_set = cur_set - first_set\n",
    "    for res in list(cur_set):\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(cur_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8541396-4b7c-4e35-bdb8-560d8a55884e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3962157964706421 - select :303,286,304,319,348,346,255,308,350,253,329,307,287,300,331,291,290,261,333,262,317,288\n",
      "0.003320834832265973 - select :266,254,265,242,246\n",
      "0.0008981181890703738 - select :353,321,326,355,328,327,318,352\n"
     ]
    }
   ],
   "source": [
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    for res in top[-1]:\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(f\"{top[0]} - {cur_res[:-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e2f19f-d255-4f11-bd90-67a7f59877b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select :\n",
      "select :266,254,265,242,246,\n",
      "select :321,326,328,353,355,327,318,352,\n"
     ]
    }
   ],
   "source": [
    "first_set = set(x for x in top3[0][-1])\n",
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    cur_set = cur_set = set(x for x in top[-1])\n",
    "    cur_set = cur_set - first_set\n",
    "    for res in list(cur_set):\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(cur_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b338b69-c246-4170-856a-a1259a82a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../prot_bert_allosteric were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../prot_bert_allosteric and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04398220777511597 - select :78,16,79,17,13,21,71,75,22,18,20\n",
      "0.00419106287881732 - select :63,62,8,86,60,88\n",
      "0.0014705873327329755 - select :59,66,34,57,67,65,90,58,33,35\n",
      "select :\n",
      "select :63,62,8,86,60,88,\n",
      "select :59,66,34,57,67,65,90,58,33,35,\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import requests\n",
    "import glob, os, math\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import xgboost as xgb\n",
    "\n",
    "from utils.extract_sequence import extract_sequence\n",
    "from utils.pocket_feature import pocket_feature\n",
    "from utils.sequence_indices import sequence_indices\n",
    "from utils.pocket_coordinates import pocket_coordinates\n",
    "\n",
    "N_ATOMS = 9\n",
    "MODEL_PATH = \"../prot_bert_allosteric\"\n",
    "base_url = \"https://files.rcsb.org/download\"\n",
    "pdb_dir = \"../data/pdbs/\"\n",
    "pocket_dir = \"../data/pockets/\"\n",
    "pdb_id = \"3LNY\"\n",
    "chain_id = \"A\"\n",
    "\n",
    "is_test = False\n",
    "\n",
    "pdb_path = os.path.join(pdb_dir, f\"{pdb_id}.pdb\")\n",
    "pocket_path = os.path.join(pocket_dir, f\"{pdb_id}_out\")\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    ASD_path = \"../data/source_data/ASD_Release_201909_AS.txt\"\n",
    "\n",
    "    asd = None\n",
    "    with open(ASD_path, \"r\") as f:\n",
    "        asd = f.readlines()\n",
    "\n",
    "    mod_id, modulator, residues = None, None, None\n",
    "    for line in asd[1:]:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        pdb, modulator, chain_id, mod_id = line[4], line[6], line[7], line[11]\n",
    "\n",
    "        if pdb != pdb_id:\n",
    "            continue\n",
    "\n",
    "        if len(set(chain_id.split(\";\"))) != 1:\n",
    "            continue\n",
    "        chain_id = chain_id[0]\n",
    "\n",
    "        if len(set(modulator.split(\";\"))) != 1:\n",
    "            continue\n",
    "        modulator = modulator.split(\";\")[0]\n",
    "\n",
    "        # extract residues\n",
    "        res_raw = [\n",
    "            res.replace(\":\", \",\").split(\",\") for res in line[-1].split(\"; \")\n",
    "        ]\n",
    "        # residue_clean format: chain id + residue type + residue number\n",
    "        residues = [\n",
    "            [res[0][-1], ch[:3], ch[3:]] for res in res_raw for ch in res[1:]\n",
    "        ]\n",
    "        # select only residues in the same chain of modulator\n",
    "        residues = [res for res in residues if res[0] == chain_id]\n",
    "\n",
    "        break\n",
    "#### Test - end ####\n",
    "\n",
    "\n",
    "if not os.path.exists(pdb_path):\n",
    "    response = requests.get(f\"{base_url}/{pdb_id}.pdb\")\n",
    "    if response.status_code == 200:  # Check if the request was successful\n",
    "        with open(pdb_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"PDB file {pdb_id}.pdb downloaded successfully.\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download {pdb_id}.pdb. Check if the PDB ID is correct.\")\n",
    "\n",
    "sequence = extract_sequence(pdb_path, chain_id)\n",
    "\n",
    "if len(sequence) <= 10:\n",
    "    raise Exception(\"Sequence is too short.\")\n",
    "\n",
    "if not os.path.exists(pocket_path):\n",
    "    os.system(f\"fpocket -f {pdb_path} -k {chain_id}\")\n",
    "    os.system(f\"mv {os.path.join(pdb_dir, pdb_id)}_out {pocket_dir}\")\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    protein = None\n",
    "    lig_x, lig_y, lig_z, lig_cnt = 0, 0, 0, 0\n",
    "\n",
    "    with open(pdb_path, \"r\") as f:\n",
    "        protein = f.readlines()\n",
    "\n",
    "    for line in protein:\n",
    "        if (\n",
    "            line[:6] == \"HETATM\" and modulator == line[17:20].strip()\n",
    "            and line[21] == chain_id and mod_id == line[22:26].strip()\n",
    "        ):\n",
    "            lig_x += float(line[30:38])\n",
    "            lig_y += float(line[38:46])\n",
    "            lig_z += float(line[46:54])\n",
    "            lig_cnt += 1\n",
    "\n",
    "    lig_x /= lig_cnt\n",
    "    lig_y /= lig_cnt\n",
    "    lig_z /= lig_cnt\n",
    "#### Test - end ####\n",
    "\n",
    "pocket_names = glob.glob(f\"{pocket_path}/pockets/*.pdb\")\n",
    "pocket_names = sorted(\n",
    "    pocket_names,\n",
    "    key=lambda x: int(x.split(\"pocket\")[-1].split(\"_\")[0])\n",
    ")\n",
    "\n",
    "pockets_feats = pocket_feature(f\"{pocket_path}/{pdb_id}_info.txt\")\n",
    "selected_idxs = []\n",
    "pocket_residue_indices = []\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    atomTarget = {}\n",
    "    for res in residues:\n",
    "        atomTarget[f'{res[1]}{res[2]}'] = res[0]\n",
    "\n",
    "    dists = []\n",
    "    countsPockets = [] # for atom count\n",
    "#### Test - end ####\n",
    "\n",
    "for idx, pocket_name in enumerate(pocket_names):\n",
    "    pocket = None\n",
    "    with open(pocket_name, \"r\") as f:\n",
    "        pocket = f.readlines()\n",
    "\n",
    "#### Test - begin ####\n",
    "    if is_test:\n",
    "        poc_x, poc_y, poc_z = 0, 0, 0\n",
    "        pocketAtomCount = 0\n",
    "#### Test - end ####\n",
    "\n",
    "    poc_cnt = 0\n",
    "    residue_indices = set()\n",
    "\n",
    "    for line in pocket:\n",
    "        if line[:4] == \"ATOM\":\n",
    "            poc_cnt += 1\n",
    "            residue_index = line[22:26].strip()\n",
    "            atom = line[17:20] + residue_index\n",
    "            residue_indices.add(residue_index)\n",
    "\n",
    "#### Test - begin ####\n",
    "            if is_test:\n",
    "                poc_x += float(line[30:38])\n",
    "                poc_y += float(line[38:46])\n",
    "                poc_z += float(line[46:54])\n",
    "                chainID = line[21]\n",
    "                if atom in atomTarget and atomTarget[atom] == chainID:\n",
    "                    pocketAtomCount += 1\n",
    "#### Test - end ####\n",
    "\n",
    "    if poc_cnt == 0:\n",
    "        continue\n",
    "\n",
    "#### Test - begin ####\n",
    "    if is_test:\n",
    "        poc_x /= poc_cnt\n",
    "        poc_y /= poc_cnt\n",
    "        poc_z /= poc_cnt\n",
    "        dist = math.sqrt(\n",
    "            (poc_x - lig_x) ** 2 + (poc_y - lig_y) ** 2 +\n",
    "            (poc_z - lig_z) ** 2\n",
    "        )\n",
    "\n",
    "        dists.append(dist)\n",
    "        countsPockets.append(pocketAtomCount)\n",
    "#### Test - end ####\n",
    "\n",
    "    selected_idxs.append(idx)\n",
    "    pocket_residue_indices.append(list(residue_indices))\n",
    "\n",
    "if len(selected_idxs) <= 2:\n",
    "    raise Exception(\"Too few pockets extracted.\")\n",
    "\n",
    "pocket_features = [pockets_feats[idx] for idx in selected_idxs]\n",
    "\n",
    "seq_indices = sequence_indices(pdb_id, chain_id)\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    dist_min_idx = np.argmin(dists)\n",
    "    labels = [1 if item >= N_ATOMS else 0 for item in countsPockets] # for atom count\n",
    "    labels[dist_min_idx] = 1\n",
    "\n",
    "    seq_labels = ['N'] * len(sequence)\n",
    "    for i in range(len(labels)):\n",
    "            if labels[i] == 1:\n",
    "                for residue_index in pocket_residue_indices[i]:\n",
    "                    if residue_index in seq_indices and seq_indices[residue_index] < len(sequence):\n",
    "                        seq_labels[seq_indices[residue_index]] = 'Y'\n",
    "#### Test - end ####\n",
    "\n",
    "pocket_coord = pocket_coordinates(pdb_path, f\"{pocket_path}/pockets/\", pdb_id, chain_id, pocket_residue_indices)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, do_lower_case=False )\n",
    "model = BertModel.from_pretrained(MODEL_PATH)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "seq_emb = None\n",
    "poc_res_emb = []\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    poc_labels = []\n",
    "#### Test - end ####\n",
    "\n",
    "with torch.no_grad():\n",
    "    seq = \" \".join(sequence)\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        [seq],\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    input_ids = torch.tensor(encoding['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(encoding['attention_mask']).to(device)\n",
    "    embedding = model(input_ids=input_ids, attention_mask=attention_mask)[0].cpu().numpy()\n",
    "\n",
    "    seq_len = (attention_mask[0] == 1).sum()\n",
    "    token_emb = embedding[0][1:seq_len-1]\n",
    "    seq_emb = token_emb.mean(axis=0)\n",
    "\n",
    "    for i in range(len(pocket_residue_indices)):\n",
    "        add_pocket = True\n",
    "        cur_poc_emb = []\n",
    "\n",
    "#### Test - begin ####\n",
    "        if is_test:\n",
    "            poc_labels.append(labels[i])\n",
    "#### Test - end ####\n",
    "\n",
    "        for idx in pocket_residue_indices[i]:\n",
    "            try:\n",
    "                token = token_emb[seq_indices[idx]]\n",
    "                cur_poc_emb.append(token)\n",
    "            except Exception as e:\n",
    "                add_pocket = False\n",
    "#### Test - begin ####\n",
    "                if is_test:\n",
    "                    poc_labels.pop()\n",
    "#### Test - end ####\n",
    "                break\n",
    "        \n",
    "        if add_pocket:\n",
    "            poc_res_emb.append(cur_poc_emb)\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "def get_res_data(poc_res_emb, pocket_coord):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(min(len(poc_res_emb), len(pocket_coord))):\n",
    "        seq_emb = []\n",
    "        for res_idx in range(min(len(poc_res_emb[i]), len(pocket_coord[i]))):\n",
    "            seq_emb.append(poc_res_emb[i][res_idx])\n",
    "        seq_emb = np.array(seq_emb).mean(axis=0)\n",
    "        poc = pocket_features[i]\n",
    "        X.append(np.concatenate((seq_emb, poc)))\n",
    "#### Test - begin ####\n",
    "        if is_test:\n",
    "            Y.append(labels[i])\n",
    "#### Test - end ####\n",
    "\n",
    "    if is_test:\n",
    "        return X, Y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "if is_test:\n",
    "    X_Test, Y_Test = get_res_data(poc_res_emb, pocket_coord)\n",
    "    dtest = xgb.DMatrix(X_Test, label=Y_Test)\n",
    "else:\n",
    "    X_Test = get_res_data(poc_res_emb, pocket_coord)\n",
    "    dtest = xgb.DMatrix(X_Test)\n",
    "bst = xgb.Booster()\n",
    "bst.load_model('./xgboost.model')\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "if is_test:\n",
    "    paired = list(zip(y_pred, Y_Test, pocket_residue_indices))\n",
    "else:\n",
    "    paired = list(zip(y_pred, pocket_residue_indices))\n",
    "paired_sorted = sorted(paired, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "top3 = [paired_sorted[i] for i in range(min(len(paired_sorted), 3))]\n",
    "\n",
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    for res in top[-1]:\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(f\"{top[0]} - {cur_res[:-1]}\")\n",
    "    \n",
    "first_set = set(x for x in top3[0][-1])\n",
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    cur_set = cur_set = set(x for x in top[-1])\n",
    "    cur_set = cur_set - first_set\n",
    "    for res in list(cur_set):\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(cur_res[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482e6c7c-bac2-45d5-a28f-ceb47b3ba80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/passer/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at ../prot_bert_allosteric were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../prot_bert_allosteric and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999921321868896 - 1 - select :119,109,15,17,117,108,110\n",
      "0.143299400806427 - 0 - select :18,22,17,24,23,110,19\n",
      "0.0005511316703632474 - 0 - select :115,79,116,112,86,113,111,88,75\n",
      "select \n",
      "select :18,22,24,23,19\n",
      "select :115,79,116,112,86,113,111,88,75\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "import requests\n",
    "import glob, os, math\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import xgboost as xgb\n",
    "\n",
    "from utils.extract_sequence import extract_sequence\n",
    "from utils.pocket_feature import pocket_feature\n",
    "from utils.sequence_indices import sequence_indices\n",
    "from utils.pocket_coordinates import pocket_coordinates\n",
    "\n",
    "N_ATOMS = 9\n",
    "MODEL_PATH = \"../prot_bert_allosteric\"\n",
    "base_url = \"https://files.rcsb.org/download\"\n",
    "pdb_dir = \"../data/pdbs/\"\n",
    "pocket_dir = \"../data/pockets/\"\n",
    "pdb_id = \"3HJ0\"\n",
    "chain_id = \"A\"\n",
    "\n",
    "is_test = True\n",
    "\n",
    "pdb_path = os.path.join(pdb_dir, f\"{pdb_id}.pdb\")\n",
    "pocket_path = os.path.join(pocket_dir, f\"{pdb_id}_out\")\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    ASD_path = \"../data/source_data/ASD_Release_201909_AS.txt\"\n",
    "\n",
    "    asd = None\n",
    "    with open(ASD_path, \"r\") as f:\n",
    "        asd = f.readlines()\n",
    "\n",
    "    mod_id, modulator, residues = None, None, None\n",
    "    for line in asd[1:]:\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        pdb, modulator, chain_id, mod_id = line[4], line[6], line[7], line[11]\n",
    "\n",
    "        if pdb != pdb_id:\n",
    "            continue\n",
    "\n",
    "        if len(set(chain_id.split(\";\"))) != 1:\n",
    "            continue\n",
    "        chain_id = chain_id[0]\n",
    "\n",
    "        if len(set(modulator.split(\";\"))) != 1:\n",
    "            continue\n",
    "        modulator = modulator.split(\";\")[0]\n",
    "\n",
    "        # extract residues\n",
    "        res_raw = [\n",
    "            res.replace(\":\", \",\").split(\",\") for res in line[-1].split(\"; \")\n",
    "        ]\n",
    "        # residue_clean format: chain id + residue type + residue number\n",
    "        residues = [\n",
    "            [res[0][-1], ch[:3], ch[3:]] for res in res_raw for ch in res[1:]\n",
    "        ]\n",
    "        # select only residues in the same chain of modulator\n",
    "        residues = [res for res in residues if res[0] == chain_id]\n",
    "\n",
    "        break\n",
    "#### Test - end ####\n",
    "\n",
    "\n",
    "if not os.path.exists(pdb_path):\n",
    "    response = requests.get(f\"{base_url}/{pdb_id}.pdb\")\n",
    "    if response.status_code == 200:  # Check if the request was successful\n",
    "        with open(pdb_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"PDB file {pdb_id}.pdb downloaded successfully.\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download {pdb_id}.pdb. Check if the PDB ID is correct.\")\n",
    "\n",
    "sequence = extract_sequence(pdb_path, chain_id)\n",
    "\n",
    "if len(sequence) <= 10:\n",
    "    raise Exception(\"Sequence is too short.\")\n",
    "\n",
    "if not os.path.exists(pocket_path):\n",
    "    os.system(f\"fpocket -f {pdb_path} -k {chain_id}\")\n",
    "    os.system(f\"mv {os.path.join(pdb_dir, pdb_id)}_out {pocket_dir}\")\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    protein = None\n",
    "    lig_x, lig_y, lig_z, lig_cnt = 0, 0, 0, 0\n",
    "\n",
    "    with open(pdb_path, \"r\") as f:\n",
    "        protein = f.readlines()\n",
    "\n",
    "    for line in protein:\n",
    "        if (\n",
    "            line[:6] == \"HETATM\" and modulator == line[17:20].strip()\n",
    "            and line[21] == chain_id and mod_id == line[22:26].strip()\n",
    "        ):\n",
    "            lig_x += float(line[30:38])\n",
    "            lig_y += float(line[38:46])\n",
    "            lig_z += float(line[46:54])\n",
    "            lig_cnt += 1\n",
    "\n",
    "    lig_x /= lig_cnt\n",
    "    lig_y /= lig_cnt\n",
    "    lig_z /= lig_cnt\n",
    "#### Test - end ####\n",
    "\n",
    "pocket_names = glob.glob(f\"{pocket_path}/pockets/*.pdb\")\n",
    "pocket_names = sorted(\n",
    "    pocket_names,\n",
    "    key=lambda x: int(x.split(\"pocket\")[-1].split(\"_\")[0])\n",
    ")\n",
    "\n",
    "pockets_feats = pocket_feature(f\"{pocket_path}/{pdb_id}_info.txt\")\n",
    "selected_idxs = []\n",
    "pocket_residue_indices = []\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    atomTarget = {}\n",
    "    for res in residues:\n",
    "        atomTarget[f'{res[1]}{res[2]}'] = res[0]\n",
    "\n",
    "    dists = []\n",
    "    countsPockets = [] # for atom count\n",
    "#### Test - end ####\n",
    "\n",
    "for idx, pocket_name in enumerate(pocket_names):\n",
    "    pocket = None\n",
    "    with open(pocket_name, \"r\") as f:\n",
    "        pocket = f.readlines()\n",
    "\n",
    "#### Test - begin ####\n",
    "    if is_test:\n",
    "        poc_x, poc_y, poc_z = 0, 0, 0\n",
    "        pocketAtomCount = 0\n",
    "#### Test - end ####\n",
    "\n",
    "    poc_cnt = 0\n",
    "    residue_indices = set()\n",
    "\n",
    "    for line in pocket:\n",
    "        if line[:4] == \"ATOM\":\n",
    "            poc_cnt += 1\n",
    "            residue_index = line[22:26].strip()\n",
    "            atom = line[17:20] + residue_index\n",
    "            residue_indices.add(residue_index)\n",
    "\n",
    "#### Test - begin ####\n",
    "            if is_test:\n",
    "                poc_x += float(line[30:38])\n",
    "                poc_y += float(line[38:46])\n",
    "                poc_z += float(line[46:54])\n",
    "                chainID = line[21]\n",
    "                if atom in atomTarget and atomTarget[atom] == chainID:\n",
    "                    pocketAtomCount += 1\n",
    "#### Test - end ####\n",
    "\n",
    "    if poc_cnt == 0:\n",
    "        continue\n",
    "\n",
    "#### Test - begin ####\n",
    "    if is_test:\n",
    "        poc_x /= poc_cnt\n",
    "        poc_y /= poc_cnt\n",
    "        poc_z /= poc_cnt\n",
    "        dist = math.sqrt(\n",
    "            (poc_x - lig_x) ** 2 + (poc_y - lig_y) ** 2 +\n",
    "            (poc_z - lig_z) ** 2\n",
    "        )\n",
    "\n",
    "        dists.append(dist)\n",
    "        countsPockets.append(pocketAtomCount)\n",
    "#### Test - end ####\n",
    "\n",
    "    selected_idxs.append(idx)\n",
    "    pocket_residue_indices.append(list(residue_indices))\n",
    "\n",
    "if len(selected_idxs) <= 2:\n",
    "    raise Exception(\"Too few pockets extracted.\")\n",
    "\n",
    "pocket_features = [pockets_feats[idx] for idx in selected_idxs]\n",
    "\n",
    "seq_indices = sequence_indices(pdb_id, chain_id)\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    dist_min_idx = np.argmin(dists)\n",
    "    labels = [1 if item >= N_ATOMS else 0 for item in countsPockets] # for atom count\n",
    "    labels[dist_min_idx] = 1\n",
    "\n",
    "    seq_labels = ['N'] * len(sequence)\n",
    "    for i in range(len(labels)):\n",
    "            if labels[i] == 1:\n",
    "                for residue_index in pocket_residue_indices[i]:\n",
    "                    if residue_index in seq_indices and seq_indices[residue_index] < len(sequence):\n",
    "                        seq_labels[seq_indices[residue_index]] = 'Y'\n",
    "#### Test - end ####\n",
    "\n",
    "pocket_coord = pocket_coordinates(pdb_path, f\"{pocket_path}/pockets/\", pdb_id, chain_id, pocket_residue_indices)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, do_lower_case=False )\n",
    "model = BertModel.from_pretrained(MODEL_PATH)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "seq_emb = None\n",
    "poc_res_emb = []\n",
    "\n",
    "#### Test - begin ####\n",
    "if is_test:\n",
    "    poc_labels = []\n",
    "#### Test - end ####\n",
    "\n",
    "with torch.no_grad():\n",
    "    seq = \" \".join(sequence)\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        [seq],\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    input_ids = torch.tensor(encoding['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(encoding['attention_mask']).to(device)\n",
    "    embedding = model(input_ids=input_ids, attention_mask=attention_mask)[0].cpu().numpy()\n",
    "\n",
    "    seq_len = (attention_mask[0] == 1).sum()\n",
    "    token_emb = embedding[0][1:seq_len-1]\n",
    "    seq_emb = token_emb.mean(axis=0)\n",
    "\n",
    "    for i in range(len(pocket_residue_indices)):\n",
    "        add_pocket = True\n",
    "        cur_poc_emb = []\n",
    "\n",
    "#### Test - begin ####\n",
    "        if is_test:\n",
    "            poc_labels.append(labels[i])\n",
    "#### Test - end ####\n",
    "\n",
    "        for idx in pocket_residue_indices[i]:\n",
    "            try:\n",
    "                token = token_emb[seq_indices[idx]]\n",
    "                cur_poc_emb.append(token)\n",
    "            except Exception as e:\n",
    "                add_pocket = False\n",
    "#### Test - begin ####\n",
    "                if is_test:\n",
    "                    poc_labels.pop()\n",
    "#### Test - end ####\n",
    "                break\n",
    "        \n",
    "        if add_pocket:\n",
    "            poc_res_emb.append(cur_poc_emb)\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "def get_res_data(poc_res_emb, pocket_coord):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(min(len(poc_res_emb), len(pocket_coord))):\n",
    "        seq_emb = []\n",
    "        for res_idx in range(min(len(poc_res_emb[i]), len(pocket_coord[i]))):\n",
    "            seq_emb.append(poc_res_emb[i][res_idx])\n",
    "        seq_emb = np.array(seq_emb).mean(axis=0)\n",
    "        poc = pocket_features[i]\n",
    "        X.append(np.concatenate((seq_emb, poc)))\n",
    "#### Test - begin ####\n",
    "        if is_test:\n",
    "            Y.append(labels[i])\n",
    "#### Test - end ####\n",
    "\n",
    "    if is_test:\n",
    "        return X, Y\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "if is_test:\n",
    "    X_Test, Y_Test = get_res_data(poc_res_emb, pocket_coord)\n",
    "    dtest = xgb.DMatrix(X_Test, label=Y_Test)\n",
    "else:\n",
    "    X_Test = get_res_data(poc_res_emb, pocket_coord)\n",
    "    dtest = xgb.DMatrix(X_Test)\n",
    "bst = xgb.Booster()\n",
    "bst.load_model('./xgboost.model')\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "if is_test:\n",
    "    paired = list(zip(y_pred, Y_Test, pocket_residue_indices))\n",
    "else:\n",
    "    paired = list(zip(y_pred, pocket_residue_indices))\n",
    "paired_sorted = sorted(paired, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "top3 = [paired_sorted[i] for i in range(min(len(paired_sorted), 3))]\n",
    "\n",
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    for res in top[-1]:\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(f\"{top[0]} - {top[1]} - {cur_res[:-1]}\")\n",
    "    \n",
    "first_set = set(x for x in top3[0][-1])\n",
    "for top in top3:\n",
    "    cur_res = \"select :\"\n",
    "    cur_set = cur_set = set(x for x in top[-1])\n",
    "    cur_set = cur_set - first_set\n",
    "    for res in list(cur_set):\n",
    "        cur_res = f\"{cur_res}{res},\"\n",
    "    print(cur_res[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaeabaf-8261-4ae2-b416-4063444463e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allostery",
   "language": "python",
   "name": "allostery"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
